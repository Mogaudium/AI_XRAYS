{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c02967-81d4-4edd-a38c-aca22b93ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy library for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Import the OpenCV library for computer vision tasks\n",
    "import cv2\n",
    "\n",
    "# Import the os library to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Import TensorFlow, a machine learning and neural network library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import Keras, a high-level neural networks API running on top of TensorFlow\n",
    "import keras\n",
    "\n",
    "# From the keras.layers module, import various layers used in neural network architectures\n",
    "from keras.layers import Conv2D, Flatten, BatchNormalization, Dense, MaxPooling2D, Dropout\n",
    "\n",
    "# Import train_test_split function to divide data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import to_categorical for converting a class vector to binary class matrix\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Import ImageDataGenerator from TensorFlow Keras for real-time data augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Import LabelEncoder for encoding labels with a value between 0 and n_classes-1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import Adam optimizer, a method for stochastic optimization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Import Sequential from Keras, a linear stack of neural network layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Import regularizers from TensorFlow Keras, providing penalties on layer parameters during optimization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Import various performance metrics from sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "# Import confusion_matrix to evaluate the accuracy of a classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Import seaborn, a statistical data visualization library based on matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# Import matplotlib.pyplot for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd4d447-0dad-471c-8475-6aa263f9f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "data_dir = \"C:/Users/ipman/OneDrive/Desktop/Data Mining/assignment2/archive/chest_xray/train\" # Hardcoded path on my local machine, relative path sometimes outputs error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1abcc1a-8046-4bb5-b303-5c1758c4b8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Accessing classes in data directory\n",
    "sub_folders = os.listdir(data_dir)\n",
    "print(len(sub_folders))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431ae0f-58a5-45d6-9fd1-99ee4c2d44e6",
   "metadata": {},
   "source": [
    "NORMAL and PNEUMONIA subfolders in \"train\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3756a7c3-f3be-4a34-96fc-f9c4de3637f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list named 'images' to store image data\n",
    "images = []\n",
    "\n",
    "# Create an empty list named 'labels' to store the labels corresponding to the images\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97529322-84f0-47d9-8155-7cb3cbcf1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_folder in sub_folders:\n",
    "    label = sub_folder\n",
    "    \n",
    "    # Constructing the path to the current sub_folder\n",
    "    path = os.path.join(data_dir, sub_folder)\n",
    "    \n",
    "    # Listing all the images in the sub_folder\n",
    "    sub_folder_images = os.listdir(path)\n",
    "    \n",
    "    for image_name in sub_folder_images:\n",
    "        \n",
    "        # Constructing the path of current image\n",
    "        image_path = os.path.join(path, image_name)\n",
    "        \n",
    "        # Loading the image using OpenCV\n",
    "        img = cv2.imread(image_path)\n",
    "        \n",
    "        # Resizing image\n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        \n",
    "        # Append the images to the image list \n",
    "        images.append(img)\n",
    "        \n",
    "        # Append the labels\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a65c2ff-8b02-49c8-a41b-587d1a85759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of images and labels to the numpy array\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c26f62-3864-485b-942d-71bc86b7ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93fabad2-f263-4133-bf96-56d4d77f3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "def preprocessing(img):\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfb297e1-abea-4cb3-a4d4-8fa2df32e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the preprocessing to the entire dataset\n",
    "X_train = np.array(list(map(preprocessing, X_train)))\n",
    "X_val = np.array(list(map(preprocessing, X_val)))\n",
    "X_test = np.array(list(map(preprocessing, X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b7e36eb-a0ec-4b59-a849-f45b9f82218b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (C:\\Users\\ipman\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\preprocessing\\image\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import the ImageDataGenerator class from Keras for data preprocessing\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Data Augmentation\u001b[39;00m\n\u001b[0;32m      5\u001b[0m data_gen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m      6\u001b[0m     width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,  \u001b[38;5;66;03m# Maximum shift in the width of the image, as a fraction of total width (10% here)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     height_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;66;03m# Maximum shift in the height of the image, as a fraction of total height (10% here)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,      \u001b[38;5;66;03m# Maximum degree of random rotations of the image\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     shear_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m         \u001b[38;5;66;03m# Shear Intensity (Shear angle in counter-clockwise direction as radians)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (C:\\Users\\ipman\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\preprocessing\\image\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import the ImageDataGenerator class from Keras for data preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data Augmentation\n",
    "data_gen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,  # Maximum shift in the width of the image, as a fraction of total width (10% here)\n",
    "    height_shift_range=0.1, # Maximum shift in the height of the image, as a fraction of total height (10% here)\n",
    "    rotation_range=10,      # Maximum degree of random rotations of the image\n",
    "    shear_range=0.1         # Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fa936-ff26-4187-b8f7-acf0d7b9a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0f1be1-fde6-4c6a-80b1-80963fc30d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634b5f3-cc7c-46ea-a83c-ea6159d4fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Classes\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a2765-fdbd-405c-9360-781646e89528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the labels into One-Hot encoding\n",
    "y_train = to_categorical(y_train, num_classes = num_classes)\n",
    "y_val = to_categorical(y_val, num_classes = num_classes)\n",
    "y_test = to_categorical(y_test, num_classes = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a78a7-6d52-4857-8a88-b17746724dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the CNN model\n",
    "def build_model():\n",
    "    model = Sequential()  # Initialize a sequential model\n",
    "\n",
    "    # First convolutional layer with 32 filters of size 5x5, using same padding, stride of 1, and ReLU activation\n",
    "    model.add(Conv2D(32, (5,5), strides=(1,1), padding='same', activation='relu', input_shape=(256, 256, 3)))\n",
    "    # First max pooling layer with pool size of 2x2\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    # Add dropout layer to prevent overfitting, dropping out 50% of the units\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Second convolutional layer with 64 filters of size 3x3, same padding, stride of 1, and ReLU activation\n",
    "    model.add(Conv2D(64, (3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Third convolutional layer with 128 filters, same settings\n",
    "    model.add(Conv2D(128, (3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Fourth convolutional layer with 256 filters\n",
    "    model.add(Conv2D(256, (3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Fifth convolutional layer with 512 filters\n",
    "    model.add(Conv2D(512, (3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Sixth convolutional layer with 32 filters (reducing complexity perhaps for specific feature extraction)\n",
    "    model.add(Conv2D(32, (3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Flatten the output of the last pooling layer to feed into the dense layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layer with 512 neurons and ReLU activation\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Another dense layer with 1024 neurons\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output layer with a number of neurons equal to the number of classes, using sigmoid activation\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    \n",
    "    # Optimizer configuration - using Adam optimizer with a learning rate of 0.0001\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    # Compile the model with binary crossentropy loss and accuracy as the metric\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a36d5d-5032-403e-a1e8-d0dcc36a58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model() # Call the build_model function to create and configure the CNN model\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec36fb2-fc36-4e75-a0d3-dbb7c5fa0bbb",
   "metadata": {},
   "source": [
    "The model has a total of 2,507,618 parameters, all of which are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b20743-574c-4bde-bb75-ec8a26b09a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debbuging / Checking compatibility for tensorflow GPU computing\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"All devices:\", tf.config.list_physical_devices())\n",
    "else:\n",
    "    print(\"No GPU is detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10687df1-d806-4171-bdb9-cb8a6cf402c9",
   "metadata": {},
   "source": [
    "I have tried CPU and GPU computing to see if there is any difference in results, there is none.\n",
    "\n",
    "<b> NOTE THAT MY CPU IS OVERCLOCKED, its a beast :) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b694f1f-05bd-4a1a-baf2-f097c07cc9d8",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(data_gen.flow(X_train, y_train, batch_size = 32),\n",
    "                   validation_data = (X_val, y_val),\n",
    "                   epochs = 30,\n",
    "                   verbose = 2)\n",
    "\n",
    "model.save('my_model.keras') # Saving the model in order to reuse it without training the model again\n",
    "\n",
    "history_dict = history.history # Saving the history in order to reuse it without training the model again\n",
    "\n",
    "with open('history.json', 'w') as f:\n",
    "    json.dump(history_dict, f)\n",
    "\n",
    "<b>RUN THIS CELL AGAIN IF YOU WANT TO TRAIN THE MODEL AGAIN, OTHERWISE, I HAVE SAVED THE MODEL AS \"my_model\" AND HISTORY AS \"history\".</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebca1a2-831f-4244-8e24-aa01fcdfad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model # Loading saved model\n",
    "\n",
    "\n",
    "model = load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d35a7f-f13f-457f-bf4c-281354e65529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # Loading saved history \n",
    "\n",
    "with open('history.json', 'r') as f:\n",
    "    history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2e083-5265-456b-928b-5b85710756c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    # Check if 'history' is a dict or has a .history attribute\n",
    "    if isinstance(history, dict):\n",
    "        hist = history  # If already a dict, use it directly\n",
    "    else:\n",
    "        hist = history.history  # If a history object, use the .history attribute\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(hist['loss'])\n",
    "    plt.plot(hist['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    for i, loss in enumerate(hist['val_loss']):\n",
    "        plt.text(i, loss, f'{loss:.2f}', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(hist['accuracy'])\n",
    "    plt.plot(hist['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    for i, acc in enumerate(hist['val_accuracy']):\n",
    "        plt.text(i, acc, f'{acc:.2f}', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_accuracy(history)\n",
    "\n",
    "# plot_loss_accuracy(model_history)  # Use this if history is from model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da1ee21-bb02-419c-a704-6ef3f5091f18",
   "metadata": {},
   "source": [
    "Model Loss:\n",
    "\n",
    "This graph displays the loss metric of the model on both the training set and the validation set.\n",
    "\n",
    "Both losses decrease over time, which is a good indicator that the model is learning.\n",
    "\n",
    "Initially, the training loss decreases more sharply than the validation loss, indicating the model is quickly learning from the training data.\n",
    "\n",
    "After the sharp decrease, the validation loss flattens and remains relatively constant, suggesting that the model might not be improving much on the validation set beyond this point.\n",
    "\n",
    "The training loss continues to slightly decrease or remains flat, which could be a sign that the model has nearly converged on the training data.\n",
    "\n",
    "Throughout the process, the validation loss remains higher than the training loss, which is typical in most training scenarios due to the model being optimized on the training data.\n",
    "\n",
    "Model Accuracy:\n",
    "\n",
    "The second graph shows the accuracy metric for both the training set and validation set.\n",
    "\n",
    "The accuracy on both datasets increases sharply at the beginning, which indicates a good initial learning phase.\n",
    "\n",
    "The training accuracy continues to increase and plateaus near a high value, which indicates a good fit on the training data.\n",
    "\n",
    "The validation accuracy increases along with the training accuracy and then fluctuates slightly around a certain value but generally remains flat after about 15 epochs, suggesting that the model is not significantly improving after this point.\n",
    "\n",
    "There is a noticeable gap between training and validation accuracy, suggesting some overfitting. However, the gap does not widen as the epochs increase, which is a good sign.\n",
    "\n",
    "Overall:\n",
    "\n",
    "The model shows effective learning patterns with both metrics improving.\n",
    "\n",
    "The plateau in validation metrics suggests potential limits reached by the current model configuration.\n",
    "\n",
    "Slight overfitting is observed, but it doesn't worsen over time, indicating stable generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5b753-da9d-4ee1-8da0-d40b7d8a68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Converting one-hot encoded test labels back to categorical labels\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Generating the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Visualizing the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Pneumonia'], yticklabels=['Normal', 'Pneumonia'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf830f-2b17-46c4-a1fc-7768efcaaef8",
   "metadata": {},
   "source": [
    "True Positives (TP): The lower right cell shows 710 instances where the model correctly predicted 'Pneumonia'. This indicates a high number of correct predictions for this class.\n",
    "\n",
    "True Negatives (TN): The upper left cell shows 281 instances where the model correctly identified 'Normal'. This suggests that the model is reasonably good at identifying the negative class as well.\n",
    "\n",
    "False Positives (FP): The upper right cell has a value of 6, indicating that there were only 6 instances where the model incorrectly predicted 'Pneumonia' when the actual label was 'Normal'. This is quite low, showing that the model doesn't often mistake healthy cases for Pneumonia.\n",
    "\n",
    "False Negatives (FN): The lower left cell shows 47 instances where the model predicted 'Normal' but the true label was 'Pneumonia'. These are critical errors in a medical context, as they represent missed diagnoses.\n",
    "\n",
    "In summary, the confusion matrix suggests the model has a strong predictive ability, with a high number of true positives and true negatives and relatively low false positives and false negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f1e6d-df21-4d8b-9487-98399e79e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred_classes)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred_classes)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred_classes)\n",
    "\n",
    "# ROC Curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_classes)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"AUC: {roc_auc:.2f}\")\n",
    "\n",
    "# Visualization of ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce819c-0496-461f-b5ba-ab38a8c0ef06",
   "metadata": {},
   "source": [
    "ROC Curve: The solid orange line represents the ROC curve of the model, with the area under the curve (AUC) being 0.96. <b>This is a very good score as the AUC ranges from 0 to 1, with 1 representing a perfect model.</b>\n",
    "\n",
    "True Positive Rate (TPR): Also known as recall, it's on the y-axis and indicates the proportion of actual positives that are correctly identified as such.\n",
    "\n",
    "False Positive Rate (FPR): Plotted on the x-axis, it represents the proportion of actual negatives that are incorrectly identified as positives.\n",
    "\n",
    "Diagonal Dashed Line: The blue dashed line represents the ROC curve of a purely random classifier; a good classifier stays as far away from this line as possible.\n",
    "\n",
    "Performance Metrics:\n",
    "\n",
    "Accuracy: The model correctly predicts both pneumonia and normal cases 95% of the time.\n",
    "\n",
    "Precision: Out of all cases predicted as pneumonia, 99% are truly pneumonia cases.\n",
    "\n",
    "Recall: The model correctly identifies 94% of all actual pneumonia cases.\n",
    "\n",
    "F1 Score: With an F1 score of 0.96, the model has a high balance between precision and recall.\n",
    "\n",
    "<b>AUC: An AUC of 0.96 implies a high chance that the model will be able to distinguish between pneumonia and normal cases.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df895d-247c-4978-8816-ef4313867cc2",
   "metadata": {},
   "source": [
    "<b>Using LIME for Local Interpretability</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9d3fe-9528-4bb2-b94f-9b47dad9e8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lime import lime_image  # Module for explaining predictions on image data\n",
    "from skimage.segmentation import mark_boundaries  # Function to outline segments in an image\n",
    "\n",
    "# Function to explain an instance with LIME for image data\n",
    "def explain_instance_with_lime(image, model, num_features=10, hide_rest=False):\n",
    "   \n",
    "    # Create an explainer for images\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    \n",
    "    # Generate explanation for the specific image\n",
    "    explanation = explainer.explain_instance(image.astype('double'), model.predict,\n",
    "                                             top_labels=5, hide_color=0, num_samples=1000)\n",
    "\n",
    "    # Get image and mask for the top prediction\n",
    "    temp, mask = explanation.get_image_and_mask(\n",
    "        explanation.top_labels[0], positive_only=False, num_features=num_features, hide_rest=hide_rest\n",
    "    )\n",
    "\n",
    "    # Visualize the image and the mask\n",
    "    plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "    plt.title(f'LIME Explanation')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='red', edgecolor='r', label='Positive Regions'),\n",
    "                       Patch(facecolor='green', edgecolor='g', label='Negative Regions')]\n",
    "    plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "   \n",
    "explain_instance_with_lime(X_test[50], model)  # Change the vale in X_test[] to analyze a different image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f8569-f248-4ff6-9e77-776a938d140f",
   "metadata": {},
   "source": [
    "The function 'explain_instance_with_lime', utilizes the LIME framework to interpret and explain the predictions of an image-based machine learning model. The explanation aims to make the model's decision-making process transparent by identifying which parts of the input image influence the model's predictions.\n",
    "\n",
    "Red areas indicate features (or superpixels) that the model found to be positively correlated with the predicted class. \n",
    "\n",
    "Green areas  represent features that negatively influence the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567de11-2a50-4d92-b6f1-11272e409e13",
   "metadata": {},
   "source": [
    "<b>Using Grad-CAM for Visualizing Important Regions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0463da60-ad87-4c0a-96e8-875226dfe1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # Create a model that outputs the last convolutional layer and the final model predictions\n",
    "    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "\n",
    "    # Using tf.GradientTape to track operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run the model and get the outputs of the last convolutional layer and the final predictions\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        \n",
    "        # If no specific prediction index is provided, use the class with highest score\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        \n",
    "        # Get the output of the model for the class index that we're interested in\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # Compute gradients of the target class output \n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # Global average pooling of the gradients across the width and height dimensions\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # Weight the output feature map with the computed pooled gradients\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)  # Remove dimensions of size 1\n",
    "\n",
    "    # Apply ReLU to the heatmap and normalize it\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()  # Convert tensor to numpy array for visualization\n",
    "\n",
    "heatmap = make_gradcam_heatmap(np.expand_dims(X_test[50], axis=0), model, 'conv2d_5')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6d9e1-9cd5-49ca-ae24-db4f6dd9ef98",
   "metadata": {},
   "source": [
    "Model Creation: The function starts by defining a new Model that takes the same inputs as the original model but outputs both the activations from the last convolutional layer (last_conv_layer_name) and the final predictions. This allows the function to access intermediate activations needed for Grad-CAM.\n",
    "\n",
    "Gradient Computation: Using tf.GradientTape, which records operations for automatic differentiation, the function computes the gradient of the class output (which we're interested in) with respect to the activations of the last convolutional layer.\n",
    "\n",
    "Pooling Gradients: It then pools these gradients along the spatial dimensions (width and height), which gives a small set of numbers (one per channel) that represent the importance of each channel regarding the target class.\n",
    "\n",
    "Creating the Heatmap: These pooled gradients are then used to weigh the convolutional layer's output channels, and a spatial average is computed to obtain the heatmap. \n",
    "\n",
    "<b>The heatmap shows the regions of the input image that were most important for predicting the target class.</b>\n",
    "\n",
    "Post-processing: The heatmap is then processed through a ReLU function and normalized to make it visually interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014fde5b-f8f8-479e-a77f-f6a7d314d126",
   "metadata": {},
   "source": [
    "<b>Visualizing the Grad-CAM Output</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d62d2-a1a0-4b61-9817-d71b75719662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gradcam(img, heatmap, alpha=0.4):\n",
    "    # Ensure the image is scaled to [0, 255] and converted to uint8\n",
    "    img = np.uint8(255 * img)\n",
    "\n",
    "    # Convert to RGB if necessary (assuming the original images are in BGR format)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255 and convert to uint8\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Get the Jet colormap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Get RGB values from the colormap for each point in the heatmap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Resize the heatmap to match the image size\n",
    "    jet_heatmap = cv2.resize(jet_heatmap, (img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = np.uint8(255 * jet_heatmap)  # Ensure jet_heatmap is uint8\n",
    "\n",
    "    # Combine the heatmap with the original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "\n",
    "    # Display the superimposed image\n",
    "    plt.imshow(superimposed_img / 255)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Call the display function with the test image and heatmap\n",
    "display_gradcam(X_test[50], heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f514ee-9d79-4049-a4d7-e3a3c7543095",
   "metadata": {},
   "source": [
    "Image Scaling and Conversion: The image is first scaled to the range [0, 255] and converted to 8-bit integers, as this is the expected format for images in many visualization and image processing functions.\n",
    "\n",
    "Color Space Conversion: Converts the image from BGR (default color space for images loaded by OpenCV) to RGB to match the color order used by Matplotlib.\n",
    "\n",
    "Heatmap Scaling: The heatmap, which is originally in a floating-point format where values range from 0 to 1, is scaled to [0, 255] to match the color depth of standard images.\n",
    "\n",
    "Colormap Application: The Jet colormap is applied to the heatmap. This step converts the single-channel heatmap values into a three-channel (RGB) format, which can be visualized in color.\n",
    "\n",
    "Heatmap Resizing: The heatmap is resized to match the dimensions of the original image. This is necessary to ensure that the heatmap accurately overlays the corresponding parts of the original image.\n",
    "\n",
    "Superimposing Heatmap: The colored heatmap is then superimposed on the original image. The alpha parameter controls the transparency of the heatmap overlay, allowing you to adjust how much of the original image is visible underneath the heatmap.\n",
    "\n",
    "Display: The superimposed image is displayed using Matplotlib. The division by 255 is to convert the image back into the [0, 1] range expected by Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da89bd-4bf5-450a-8cb2-98f2312a4c20",
   "metadata": {},
   "source": [
    "<b> Areas of Focus: The brightly colored areas indicate regions that the neural network focused on when making its decision. These areas are presumably where the model detected patterns or features strongly associated with the presence of pneumonia.</b>\n",
    "\n",
    "<b>Color Gradient: The colors range from blue to red, where blue represent areas of less importance and red signifies areas of high importance. The yellow and red areas are where the model is 'looking' to make its decision about the presence of pneumonia in the chest X-ray.</b>\n",
    "\n",
    "<b>Assessment of Model Behavior: The fact that the heatmap is concentrated over the lung fields is a good sign, it suggests that the model is paying attention to the relevant parts of the X-ray for diagnosing pneumonia. </b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
